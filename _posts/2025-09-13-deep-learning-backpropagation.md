---
title: Deep Learning Backpropagation
date: 2025-09-13 16:55:58 +09:00
categories: ['deep learning']
tags: ['deep learning', 'back propagation', '']
math: true
---

역전파(Backpropagation)의 개념, 왜 계산하는지, 그리고 손실 함수를 줄이는 방향으로의 파라미터 업데이트 규칙을 정리했습니다.
역전파는 연쇄법칙을 반복 적용하여 최종 스칼라 손실 $$L$$의 기울기를 입력과 매개변수에 대해 계산하는 방법입니다. 
신경망 학습에서 $$L$$을 가중치 $$(W, b)$$에 대해 미분하여 경사하강법으로 업데이트할 때 필수적입니다.

## 1) 왜 역전파를 계산하는가
- 목적은 학습 데이터에 대한 손실 $$L(\theta)$$를 감소시키는 것 입니다. 여기서 $$\theta$$는 모델의 모든 파라미터를 의미합니다.
- 기울기 $$\nabla_\theta L$$는 함수가 가장 빠르게 증가하는 방향을 가리킵니다. 따라서 손실을 줄이기 위해서는 그 반대 방향으로 파라미터를 이동합니다.
- 수치 미분은 계산량이 매우 크고 수치오차가 커지기 쉬우므로, 연쇄법칙 기반의 역전파로 정확하고 효율적으로 $$\nabla_\theta L$$을 구합니다.
- 역전파는 미분 가능한 모든 모듈을 조합한 대규모 신경망에서도 선형 시간(파라미터 수에 비례) 내에 기울기를 계산할 수 있도록 합니다.

## 2) 손실과 업데이트 방향
- 대표 손실 예시
  - 회귀: 평균제곱오차(MSE) $$L = \frac{1}{N}\sum_i \|y_i-\hat{y}_i\|^2$$
  - 분류: 교차엔트로피 $$L = -\frac{1}{N}\sum_i \sum_c y_{i,c}\log p_{i,c}$$
- 기본 업데이트 규칙(경사하강법)
  $$
  \theta_{t+1} = \theta_t - \eta \,\nabla_\theta L(\theta_t),
  $$
  여기서 $$\eta>0$$는 학습률입니다. $$\nabla_\theta L$$이 증가 방향을 의미하므로, 손실을 줄이기 위해 음의 방향으로 이동합니다.
- 미니배치 학습에서는 배치 손실의 평균 기울기를 사용합니다.
- 정규화 예시: $$\ell_2$$ 정규화 $$L_{\text{reg}}=\lambda\|W\|_2^2$$를 추가하면 전체 손실 $$L' = L + \lambda\|W\|_2^2$$, 기울기는 $$\nabla_W L' = \nabla_W L + 2\lambda W$$가 됩니다.

### 최적화 변형(간단 개요)
- 모멘텀, Adam 등은 기울기 추정과 적응적 학습률을 활용하여 수렴을 빠르게 하거나 불안정성을 줄입니다.
- 그러나 핵심은 여전히 $$\nabla_\theta L$$이며, 역전파로 이 값을 효율적으로 얻습니다.

## 3) 문제 정의와 표기
- 스칼라 함수 $$f(\mathbf{x})$$에 대해 $$\nabla f(\mathbf{x})$$를 계산합니다. 신경망에서는 일반적으로 $$f \equiv L$$입니다.
- 학습 시 데이터는 상수, 가중치는 변수로 두며, $$\frac{\partial L}{\partial W}, \frac{\partial L}{\partial b}$$를 구하는 데 집중합니다.

## 4) 연쇄법칙과 계산 그래프
- 합성함수 $$f(x,y,z)=(x+y)z$$를 $$q=x+y$$, $$f=qz$$로 두면 <br>
  $$
  \frac{\partial f}{\partial x} = \frac{\partial f}{\partial q}\frac{\partial q}{\partial x} = z,\quad
  \frac{\partial f}{\partial y} = z,\quad
  \frac{\partial f}{\partial z} = q=x+y.
  $$
- 전방패스(Forward): 입력에서 출력까지 값을 계산하고 중간 값을 캐시합니다.
- 역전파(Backward): 출력에서 입력 방향으로 각 게이트의 지역 기울기를 곱해 전달합니다.

## 5) 직관적 이해
- 각 게이트는 자신의 지역 기울기만 알고, 상위에서 전달된 기울기와 곱해 입력 기울기를 계산합니다.
- 예) 합 게이트 입력 $$[-2,5]$$, 출력 $$3$$, 지역 기울기 $$+1$$. 최종 출력의 기울기가 $$-4$$라면 두 입력의 기울기는 모두 $$-4$$입니다.

## 6) 모듈성: 시그모이드 뉴런
- 2D 뉴런 $$y=\sigma(\mathbf{w}^\top \mathbf{x}+b)$$, $$\sigma(u)=\frac{1}{1+e^{-u}}$$
- 시그모이드의 도함수 $$\sigma'(u)=\sigma(u)\bigl(1-\sigma(u)\bigr)$$
- 실무에서는 연산을 블록 단위로 묶어 수치적 안정성과 효율을 높입니다.

## 7) 실전 절차: 단계적 계산
전방(Forward)
1. 중간변수로 함수를 단계별 분해합니다.
2. 각 단계의 출력을 캐시합니다.

역전파(Backward)
1. 전방의 역순으로 미분합니다.
2. 캐시를 활용해 중복 계산을 방지합니다.
3. 분기 지점(fork)의 변수는 들어오는 기울기를 모두 누적합니다. (다변수 연쇄법칙)

## 8) 역전파 패턴(게이트별)
- Add: 입력으로 기울기를 그대로 분배합니다 $$(+1)$$.
- Max: 전방에서 최댓값이었던 입력에만 기울기를 전달합니다(1), 나머지는 0입니다.
- Multiply: 서로 스와핑된 입력값이 지역 기울기입니다.  
  $$\frac{\partial (xy)}{\partial x}=y,\ \frac{\partial (xy)}{\partial y}=x$$.  
  한 입력이 매우 작으면 작은 쪽에 더 큰 기울기가 전달될 수 있습니다.

### 스케일 효과와 전처리
- 입력 스케일을 $$1000\times$$ 키우면 기울기도 $$1000\times$$ 커질 수 있으므로, 학습률을 조정하거나 입력 표준화를 적용해야 할 수 있습니다.

## 9) 벡터/행렬 연산의 기울기 예시
- 행렬곱 $$D=WX$$, <br> ($$W\in\mathbb{R}^{m\times n}, X\in\mathbb{R}^{n\times b}, D\in\mathbb{R}^{m\times b}$$).
- 스칼라 손실 $$L$$에 대해 $$G_D=\frac{\partial L}{\partial D}$$가 주어지면 <br>
  $$
  \frac{\partial L}{\partial W} = G_D X^\top,\qquad
  \frac{\partial L}{\partial X} = W^\top G_D.
  $$
- shape matching을 점검하여 오류를 예방합니다. 기울기의 shape은 원래 텐서와 동일합니다.

## 맺음말
> 역전파는 연쇄법칙을 활용하여, 손실을 감소시키는 파라미터 업데이트를 가능하게 합니다. 
> 경사하강법을 통해, 미니배치와 정규화를 통해, 인공지능 모델을 빠르게 수렴하도록 합니다.
