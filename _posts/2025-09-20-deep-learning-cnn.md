---
title: Deep Learning Convolutional Neural Networks
date: 2025-09-20 16:59:58 +09:00
categories: ['deep learning']
tags: ['deep learning', 'CNNs']
math: true
---

컴퓨터 비전 분야에서 핵심적인 역할을 하는 Convolutional Neural Networks, 즉 CNN에 대한 강의 자료를 정리한 것입니다.
일반적인 신경망(Neural Networks)의 한계를 짚어보고, 이미지를 처리하기 위해 특화된 CNN의 아키텍처 구조, 주요 레이어의 기능, 파라미터 설정 방법, 그리고 대표적인 CNN 모델들의 발전 과정을 포괄적으로 다루고 있습니다.

## Convolutional Neural Networks 개요

일반적인 신경망은 입력 데이터를 하나의 벡터로 변환하여 처리합니다. 하지만 이러한 방식은 이미지를 처리할 때 확장성 문제가 발생합니다.
예를 들어 CIFAR-10 데이터셋의 작은 이미지(32x32x3)는 처리가 가능하지만, 더 큰 이미지(예: 200x200x3)를 처리하려면 뉴런 하나당 120,000개의 가중치가 필요하게 되어 파라미터 수가 급격히 증가하고 과적합(Overfitting) 위험이 커집니다.

반면 CNN(ConvNet)은 입력이 이미지라는 점을 가정하여 설계를 제약함으로써 아키텍처의 효율성을 높입니다. CNN의 뉴런들은 3차원 볼륨(가로, 세로, 깊이) 형태로 배열되며, 이전 레이어의 전체가 아닌 작은 국소 영역(Local region)에만 연결됩니다.
이를 통해 파라미터 수를 획기적으로 줄일 수 있습니다.

## 주요 레이어 구성

CNN은 주로 Convolutional Layer, Pooling Layer, Fully-Connected Layer의 세 가지 주요 레이어 형태로 구성됩니다.

### 1. Convolutional Layer (CONV)
CNN의 핵심 블록으로 대부분의 계산이 이곳에서 이루어집니다.
학습 가능한 필터들의 집합으로 구성되며, 각 필터는 입력 볼륨의 가로, 세로를 슬라이딩하며 내적(Dot product)을 수행합니다.

* 국소 연결성(Local Connectivity): 뉴런은 입력의 전체 깊이(Depth)를 모두 포함하지만, 공간적으로는 작은 영역(Receptive Field)에만 연결됩니다.
* 공간적 배치(Spatial Arrangement): 출력 볼륨의 크기는 깊이(Depth), 스트라이드(Stride), 제로 패딩(Zero-Padding)이라는 세 가지 하이퍼파라미터에 의해 결정됩니다.
  * 출력 크기 계산 공식: $$(W - F + 2P) / S + 1$$ (W: 입력 크기, F: 필터 크기, P: 패딩, S: 스트라이드)
* 파라미터 공유(Parameter Sharing): 파라미터 수를 제어하기 위해 사용되는 기법입니다. 하나의 깊이 슬라이스(Depth slice)에 있는 뉴런들은 모두 같은 가중치와 편향을 공유합니다. 이는 이미지가 평행 이동 불변성(Translation invariance)을 가진다는 가정에 기반합니다.

### 2. Pooling Layer (POOL)
보통 연속적인 CONV 레이어 사이에 주기적으로 삽입됩니다. 표현(Representation)의 공간적 크기를 줄여 파라미터와 계산량을 감소시키고 과적합을 제어하는 역할을 합니다.

* 가장 일반적인 형태는 2x2 필터와 스트라이드 2를 사용하는 Max Pooling입니다. 이는 입력의 가로, 세로를 절반으로 줄이며 활성값의 75%를 버립니다.
* 최근에는 풀링 레이어를 없애고 스트라이드가 있는 CONV 레이어만으로 구성하는 추세도 있습니다.

### 3. Fully-Connected Layer (FC)
일반적인 신경망과 동일하게 모든 뉴런이 이전 레이어의 모든 활성값과 연결됩니다. 주로 네트워크의 마지막 부분에서 클래스 점수를 계산하는 데 사용됩니다. 

## 아키텍처 패턴

### 일반적인 계층 구조
가장 흔한 CNN 구조는 CONV와 RELU 레이어를 몇 번 쌓은 뒤 POOL 레이어를 배치하는 패턴을 반복하고, 마지막에 FC 레이어를 배치하는 형태입니다. 
큰 필터(예: 7x7) 하나를 사용하는 것보다 작은 필터(예: 3x3)를 여러 층 쌓는 것이 비선형성을 높이고 파라미터 수를 줄이는 데 유리합니다.

### 대표적인 사례 연구
* LeNet: 1990년대 얀 르쿤이 개발하였으며 우편번호와 숫자 인식에 사용되었습니다. 
* AlexNet: 2012년 ImageNet 대회 우승 모델로, LeNet과 유사하지만 더 깊고 큰 구조를 가졌습니다. ReLU와 Dropout 등을 사용하여 성능을 높였습니다. 
* GoogLeNet: 2014년 우승 모델로 Inception 모듈을 도입하여 파라미터 수를 획기적으로 줄였습니다(AlexNet의 60M개 대비 4M개). 
* VGGNet: 2014년 준우승 모델로 네트워크의 깊이(Depth)가 성능에 미치는 영향을 증명했습니다. 3x3 컨볼루션과 2x2 풀링만으로 구성된 균일한 아키텍처가 특징입니다. 
* ResNet: 2015년 우승 모델로 스킵 연결(Skip connection)을 도입하여 매우 깊은 네트워크 학습을 가능하게 했습니다.

## 맺음말

> CNN의 기본 원리부터 구체적인 레이어의 연산 방식, 그리고 역사적으로 중요한 아키텍처들까지 포괄적으로 설명하고 있습니다. 
> CNN은 파라미터 공유와 국소 연결성을 통해 이미지 데이터 처리에 최적화된 효율적인 구조를 가집니다. CNN을 처음부터 설계하고 학습시키기보다는, ImageNet 등으로 검증된 아키텍처와 사전 학습된 모델(Pretrained model)을 활용하여 파인튜닝(Finetuning)하는 방식을 권장드립니다.
