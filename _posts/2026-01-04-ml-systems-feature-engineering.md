---
title: ML Systems Feature Engineering
date: 2026-01-04 11:23:12 +09:00
categories: ['mlops']
tags: ['mlops', 'scaling', 'discretization'] 
math: true
---

머신러닝 시스템에서 특징(feature)은 모델의 성능과 안정성을 좌우하는 핵심 요소입니다. 2014년에 발표된 Facebook의 논문 “Practical Lessons from Predicting Clicks on Ads at Facebook”에서는 올바른 특징을 갖추는 것이 모델 개발에서 가장 중요하다고 강조하였습니다. 이후 산업 현장에서 반복적으로 확인된 사실은, 기본적으로 작동하는 모델이 확보된 이후에는 하이퍼파라미터 튜닝이나 알고리즘적 기교보다도 특징을 개선하는 것이 성능 향상에 더 큰 영향을 준다는 점입니다.

아무리 최신 모델 아키텍처를 사용하더라도 입력 특징이 부적절하면 모델 성능은 크게 제한됩니다. 이 장에서는 특징 공학의 핵심 개념과 대표적인 연산들을 정리하고, 실제 운영 환경에서 반드시 고려해야 할 문제들을 다룹니다.


## Learned Features Versus Engineered Features

딥러닝은 사람이 직접 특징을 설계하지 않아도 모델이 유용한 표현을 자동으로 학습할 수 있다는 약속을 내세웁니다. 이 때문에 딥러닝은 특징 학습(feature learning)이라고도 불립니다. 텍스트나 이미지와 같은 데이터에서는 많은 특징이 자동으로 학습될 수 있으며, 과거에 비해 수작업 특징 설계의 부담이 크게 줄어들었습니다.

그러나 실제 운영 환경의 ML 시스템은 텍스트나 이미지 외에도 다양한 구조화 데이터를 함께 사용합니다. 댓글 스팸 분류 문제를 예로 들면, 댓글 내용 외에도 작성자 정보, 계정 생성 시점, 활동 패턴, 스레드의 인기 정도 등이 중요한 신호가 될 수 있습니다. 어떤 정보를 선택하고 이를 어떤 형태로 모델에 전달할지 결정하는 과정이 바로 특징 공학입니다.


## Common Feature Engineering Operations

특징 공학에서 자주 사용되는 연산으로는 결측치 처리, 스케일링, 이산화, 범주형 특징 인코딩, 특징 교차, 그리고 위치 임베딩이 있습니다. 이러한 연산들은 문제 유형과 데이터 특성에 따라 단독으로 혹은 조합하여 사용됩니다.


## Handling Missing Values

실제 데이터에서는 결측치가 매우 흔하게 발생합니다. 결측치는 발생 원인에 따라 값 자체와 관련된 경우, 다른 변수와 관련된 경우, 또는 명확한 패턴 없이 발생하는 경우로 구분됩니다.

결측치 처리 방법은 크게 삭제와 대체로 나뉩니다. 삭제는 구현이 간단하지만 중요한 정보를 잃거나 편향을 유발할 수 있습니다. 대체는 결측치를 특정 값으로 채우는 방식으로, 평균, 중앙값, 최빈값 또는 기본값을 사용할 수 있습니다. 그러나 대체 값 선택이 부적절할 경우 모델에 노이즈를 추가하거나 데이터 누수를 유발할 수 있습니다.


## Scaling

서로 다른 범위를 갖는 수치형 특징을 그대로 모델에 입력하면, 값의 크기가 큰 특징이 과도한 영향을 미칠 수 있습니다. 이를 방지하기 위해 특징 값을 유사한 범위로 맞추는 과정을 스케일링이라고 합니다.

가장 직관적인 방법은 최소–최대 스케일링입니다. 변수 x에 대해 다음과 같이 정의됩니다.

$$
x' = \frac{x - \min(x)}{\max(x) - \min(x)}
$$

이 경우 최소값은 0, 최대값은 1로 변환됩니다.

임의의 구간 $$[a, b]$$로 스케일링하고자 할 경우 다음과 같이 확장할 수 있습니다.

$$
x' = a + \frac{x - \min(x)}{\max(x) - \min(x)} (b - a)
$$

분포가 정규분포에 가깝다고 가정할 수 있는 경우에는 표준화를 사용할 수 있습니다.

$$
x' = \frac{x - \mu}{\sigma}
$$

여기서 $$\mu$$는 평균, $$\sigma$$는 표준편차입니다.

특징 분포가 심하게 치우쳐 있는 경우에는 로그 변환이 도움이 될 수 있습니다.

$$
x' = \log(x)
$$

로그 변환은 분포의 왜도를 줄이는 데 효과적이지만, 원래 값의 해석이 어려워질 수 있으므로 주의가 필요합니다.


## Discretization

이산화는 연속형 특징을 여러 구간으로 나누어 범주형 특징으로 변환하는 기법입니다. 예를 들어 연소득을 여러 구간으로 나누어 저소득, 중산층, 고소득으로 표현할 수 있습니다.

이 기법은 모델이 무한히 많은 값을 학습하는 대신 제한된 범주만 학습하도록 도와주지만, 구간 경계에서 불연속성이 발생하고 정보 손실이 생길 수 있습니다.


## Encoding Categorical Features

운영 환경에서는 범주형 특징의 값이 시간이 지나며 계속 증가하는 경우가 많습니다. 이러한 경우 고정된 인코딩 방식은 새로운 범주를 처리하지 못하는 문제를 일으킬 수 있습니다.

해시 트릭은 범주 수를 미리 알 수 없는 경우에 유용한 방법입니다. 각 범주를 해시 함수로 변환하여 고정된 해시 공간 내의 인덱스로 매핑합니다. 해시 충돌이 발생할 수 있지만, 실제 성능 저하는 제한적인 경우가 많아 산업 현장에서 널리 사용됩니다.


## Feature Crossing

특징 교차는 두 개 이상의 특징을 결합하여 새로운 특징을 생성하는 기법입니다. 이를 통해 모델이 비선형 관계를 보다 쉽게 학습할 수 있습니다.

예를 들어 결혼 여부와 자녀 수를 결합하여 새로운 특징을 만들면, 단일 특징으로는 표현하기 어려운 관계를 모델이 더 잘 학습할 수 있습니다. 다만 특징 공간이 급격히 커질 수 있으므로 과적합에 주의해야 합니다.


## Discrete and Continuous Positional Embeddings

트랜스포머와 같은 모델에서는 입력 토큰을 병렬로 처리하므로 순서 정보를 명시적으로 제공해야 합니다. 이를 위해 위치 임베딩이 사용됩니다.

학습 가능한 위치 임베딩은 각 위치를 하나의 범주로 간주하여 임베딩 벡터를 할당합니다. 단어 임베딩과 위치 임베딩은 보통 같은 차원을 가지며, 두 벡터를 더해 모델 입력으로 사용합니다.

고정 위치 임베딩은 사인과 코사인 함수를 사용하여 정의됩니다. 원래 Transformer 논문에서는 다음과 같은 형태를 사용합니다.

$$
\text{PE}_{(pos, 2i)} = \sin\left(\frac{pos}{10000^{2i / d}}\right)
$$

$$
\text{PE}_{(pos, 2i+1)} = \cos\left(\frac{pos}{10000^{2i / d}}\right)
$$

이 방식은 푸리에 특징(Fourier features)의 한 형태로 볼 수 있습니다.

연속 좌표 $$v$$에 대한 일반적인 푸리에 특징 표현은 다음과 같습니다.

$$
\gamma(v) =
\begin{bmatrix}
a_1 \cos(2\pi b_1^\top v), &
a_1 \sin(2\pi b_1^\top v), &
\ldots, &
a_m \cos(2\pi b_m^\top v), &
a_m \sin(2\pi b_m^\top v)
\end{bmatrix}^\top
$$

이러한 표현은 연속 좌표 입력에서 고주파 패턴을 학습하는 데 효과적인 것으로 알려져 있습니다.


## 맺음말

> Feature Engineering은 단순한 전처리 단계가 아니라, 문제 도메인과 데이터의 구조를 모델에 전달하는 핵심 과정입니다. 
> 딥러닝이 자동으로 특징을 학습할 수 있게 되었음에도 불구하고, 어떤 정보를 사용할지 선택하고 이를 어떻게 표현할지는 여전히 사람의 중요한 역할입니다. 

> 좋은 특징은 모델 성능뿐 아니라 일반화 성능과 안정성까지 함께 향상시킵니다. 반대로 부적절한 특징은 아무리 정교한 모델을 사용하더라도 성능을 제한하고, 운영 환경에서 심각한 문제를 초래할 수 있습니다. 
> 따라서 특징 공학은 머신러닝 시스템 설계 전반에서 지속적으로 고민해야 할 핵심 주제입니다.
