---
title: ML Systems Sampling
date: 2025-12-21 19:22:12 +09:00
categories: ['mlops']
tags: ['mlops', 'sampling', 'weighted', 'stratified', 'reservoir']
math: true
---

머신러닝 시스템에서 훈련 데이터를 다루는 일은 단순한 전처리 단계가 아니라,
모델이 **어떤 세계를 보고 학습할 것인지**를 결정하는 핵심 설계 요소입니다.
모델 구조나 손실 함수는 눈에 잘 띄지만,
실제 프로젝트에서는 데이터 수집·정제·샘플링 단계가 전체 성패를 좌우하는 경우가 많습니다.

현실의 데이터는 완성된 “데이터셋”이라기보다,
시간에 따라 분포가 변하며 계속 생성되는 “데이터 흐름”에 가깝습니다.
따라서 훈련 데이터 구성은 일회성 작업이 아니라,
시스템 요구사항과 모델 목표에 따라 반복적으로 조정되는 과정으로 이해하는 것이 바람직합니다.

## 훈련 데이터의 범위와 기본 관점

훈련 데이터는 모델 개발 과정에서 사용되는 모든 데이터
(훈련·검증·테스트 분할 포함)를 의미합니다.
이때 다음과 같은 전제를 두는 것이 안전합니다.

- 데이터는 항상 불완전합니다. 결측, 중복, 노이즈, 라벨 오류, 샘플링 편향이 존재합니다.
- 데이터는 편향을 포함할 수 있습니다. 수집·샘플링·라벨링 과정에서 유입된 편향은 모델을 통해 증폭될 수 있습니다.
- 훈련 데이터는 고정되어 있지 않습니다. 실패 사례 분석이나 성능 저하에 따라 재수집·재샘플링이 반복됩니다.

이 관점에서 샘플링은 단순히 데이터를 줄이는 기술이 아니라,
**모델이 어떤 분포를 “현실”로 인식하게 할지 결정하는 선택**입니다.

---

## Sampling

샘플링은 전체 가능한 현실 데이터에서 훈련 데이터를 고르는 과정,
주어진 데이터에서 train/val/test split을 만드는 과정,
운영 중 이벤트를 추출해 모니터링하는 과정 등 다양한 단계에서 사용됩니다.
여기서는 훈련 데이터 구성을 위한 샘플링에 초점을 둡니다.

샘플링이 필요한 대표적인 상황은 다음과 같습니다.

- 현실 세계의 모든 데이터를 확보할 수 없는 경우
- 확보한 데이터가 너무 커 전량 처리(학습·정제)가 불가능한 경우
- 빠른 실험을 위해 제한된 서브셋으로 선행 검증을 해야 하는 경우

샘플링 방법을 이해하는 목적은
“어떤 데이터를 뽑을 것인가”를 넘어서,
**그 결과로 어떤 세계에 대한 추정을 하게 되는지**를 이해하는 데 있습니다.

---

## 샘플링 편향과 추정 관점

샘플링을 데이터 선택 문제로만 보면 “몇 개를 뽑는가”로 끝나기 쉽지만,
통계적 관점에서는 “어떤 값을 어떤 세계에 대해 추정하는가”가 핵심입니다.

모집단 분포를 $$\mathcal{D}$$,
샘플링으로 관측한 데이터 분포를 $$\tilde{\mathcal{D}}$$라 하면,
실무에서는 $$\tilde{\mathcal{D}} \neq \mathcal{D}$$인 경우가 대부분입니다.
모델은 관측 분포 $$\tilde{\mathcal{D}}$$에 최적화되므로,
아무 보정 없이 평가하면 현실 세계 $$\mathcal{D}$$에서의 성능과 어긋날 수 있습니다.

어떤 함수 $$f(x)$$의 모집단 기대값은 다음과 같이 정의됩니다.

$$
\mu = \mathbb{E}_{x \sim \mathcal{D}}[f(x)]
$$

이는 “현실 세계 전체에서의 평균적인 값”을 의미합니다.
반면 단순 표본 평균은 다음과 같습니다.

$$
\hat{\mu}_{\text{naive}} = \frac{1}{n}\sum_{i=1}^{n} f(x_i)
$$

이 추정량은 표본이 모집단을 대표할 때만 $$\mu$$를 정확히 근사합니다.
샘플링이 편향되어 있다면,
이 값은 현실 세계가 아니라 **관측된 세계의 평균**을 정확히 계산한 결과가 됩니다.

이후에 다루는 층화, 가중, 중요도 샘플링은
표본 선택과 함께 **추정 단계에서 편향을 어떻게 보정할 것인가**라는 관점에서 이해하는 것이 중요합니다.

---

## Nonprobability Sampling

비확률 샘플링은 표본 선택 확률이 명시적으로 정의되지 않는 방식입니다.
대표적으로 다음과 같은 형태가 있습니다.

- 접근 가능한 데이터부터 선택하는 편의 샘플링
- 기존 표본이 연결하는 대상을 확장하는 스노우볼 샘플링
- 전문가 판단에 따른 판단 샘플링
- 슬라이스별 개수만 맞추는 할당 샘플링

이 방식들은 빠르게 데이터를 확보할 수 있다는 장점이 있지만,
표본이 모집단을 대표한다는 보장이 없어
선택 편향에 취약합니다.
초기 프로토타이핑에는 유용할 수 있으나,
신뢰 가능한 성능 추정을 목표로 한다면
확률 기반 샘플링이나 별도의 편향 보정이 필요합니다.

---

## Simple Random Sampling

단순 무작위 샘플링은 모집단의 모든 샘플이 동일한 확률로 선택되는 방식입니다.
모집단 크기를 $$N$$, 표본 크기를 $$n$$이라 할 때,
각 샘플은 대략 $$n/N$$의 확률로 선택됩니다.

이 방식의 장점은 구현이 단순하고,
표본 평균이 바로 모집단 평균의 불편 추정량이 된다는 점입니다.
반면 희소 범주가 표본에서 누락될 위험이 큽니다.

어떤 클래스가 비율 $$p$$로 존재할 때,
$$n$$개를 무작위로 뽑았을 때 한 번도 등장하지 않을 확률은 다음과 같습니다.

$$
\Pr(\text{no sample of the class}) = (1-p)^n
$$

$$p$$가 매우 작으면,
$$n$$이 충분히 크지 않은 한 해당 클래스는 표본에서 사라질 수 있습니다.
이 경우 모델은 그 클래스를 사실상 학습하지 못합니다.

---

## Stratified Sampling

층화 샘플링은 모집단을 여러 계층으로 나눈 뒤,
각 계층에서 별도로 샘플링하는 방식입니다.
희소 범주 누락을 줄이기 위해 자주 사용됩니다.

모집단을 $$H$$개의 계층으로 나누고,
계층 $$h$$의 크기를 $$N_h$$,
그 계층에서 뽑는 표본 수를 $$n_h$$라 하면,

$$
\sum_{h=1}^{H} N_h = N,\quad \sum_{h=1}^{H} n_h = n
$$

이는 전체 모집단과 표본이
각 계층의 합으로 구성된다는 정의를 나타냅니다.

비례 할당은 다음 조건을 만족합니다.

$$
\frac{n_h}{n} = \frac{N_h}{N}
$$

즉, 표본에서도 계층 비율을 모집단과 동일하게 유지합니다.
희소 계층을 더 많이 확보하고 싶다면
이 비율을 의도적으로 깨는 목적 기반 할당을 사용할 수 있습니다.
이 경우 추정 단계에서 반드시 보정이 필요합니다.

모집단 평균은 계층 평균의 가중합으로 표현됩니다.

$$
\mu = \sum_{h=1}^{H} \frac{N_h}{N}\,\mu_h
$$

표본에서 계층 평균 $$\bar{f}_h$$를 계산하면,
층화 추정량은 다음과 같습니다.

$$
\hat{\mu}_{\text{strat}} = \sum_{h=1}^{H} \frac{N_h}{N}\,\bar{f}_h
$$

이는 “각 계층 안에서는 충분히 보고,
마지막에 현실 세계의 비율로 다시 합친다”는 의미입니다.

---

## Weighted Sampling

가중 샘플링은 각 샘플 $$x_i$$에 가중치 $$w_i$$를 부여하고,
이에 비례해 선택 확률을 정하는 방식입니다.

$$
\Pr(x_i)=\frac{w_i}{\sum_j w_j}
$$

이는 “어떤 샘플을 더 자주 보게 할 것인가”를 명시적으로 설계하는 방법입니다.
가중 샘플링은 표본 선택 단계이며,
손실 함수에서의 샘플 가중치와는 구분해야 합니다.

샘플 가중치를 포함한 경험적 위험 최소화는 다음과 같이 표현됩니다.

$$
\hat{R}(\theta) = \frac{1}{\sum_{i=1}^{n} \alpha_i}\sum_{i=1}^{n}\alpha_i\,\ell(g_\theta(x_i), y_i)
$$

여기서 $$\alpha_i$$는
샘플이 학습에 미치는 상대적 중요도를 의미합니다.
선택과 보정은 반드시 동일할 필요는 없으며,
목적에 따라 독립적으로 설계될 수 있습니다.

---

## Reservoir Sampling

리저버 샘플링은 전체 크기를 알 수 없는 스트리밍 데이터에서,
항상 균등한 확률로 $$k$$개의 표본을 유지하기 위한 알고리즘입니다.

이 방식의 핵심은,
전체 데이터를 저장하지 않아도
“현재까지의 데이터에 대한 평균”을
항상 올바르게 추정할 수 있다는 점입니다.
즉, 샘플링 단계에서 균등성을 유지함으로써
추정 단계를 단순하게 만듭니다.

---

## Importance Sampling

중요도 샘플링은 목표 분포 $$P(x)$$ 대신
접근 가능한 분포 $$Q(x)$$에서 샘플링하고,
가중치로 이를 보정하는 방법입니다.

$$
\mathbb{E}_{P(x)}[f(x)]
= \mathbb{E}_{Q(x)}\left[f(x)\frac{P(x)}{Q(x)}\right]
$$

이는 “선택이 편향되었음을 인정하고,
그 비율을 명시적으로 나누어 준다”는 의미입니다.
실무에서는 $$P/Q$$를 정확히 알기 어렵기 때문에,
근사나 정규화된 형태를 사용하는 경우가 많습니다.

---

## 맺음말

> 샘플링은 단순히 데이터를 줄이는 기술이 아니라, 
> 모델이 어떤 세계를 보고, 
> 어떤 세계에 대해 책임질지를 결정하는 설계 요소입니다.

> 좋은 ML 시스템은 
> 샘플링으로 발생한 왜곡을 숨기지 않고, 
> 언제 학습을 위해 왜곡을 허용했고, 
> 언제 추정과 평가에서 이를 복원했는지를 
> 명확히 구분합니다.
