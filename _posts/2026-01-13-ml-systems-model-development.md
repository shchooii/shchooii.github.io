---
title: ML Systems Model Development & Training
date: 2026-01-13 12:00:00 +09:00
categories: ['mlops']
tags: ['mlops', 'model development', 'offline evaluation', 'ensemble']
math: true
---

모델 개발은 한 번에 끝나는 일이 아니라 반복적인(iterative) 과정입니다. 매 반복(iteration)마다 이전 실험 대비 성능이 개선되었는지 확인하고, 그 개선이 실제 프로덕션에 적합한 개선인지(비용/지연/해석가능성 등)를 함께 평가해야 합니다.

- 문제에 맞는 모델을 고르는 방법(모델 선택 팁)
- 모델 개발 과정에서 필요한 운영 요소(디버깅, 실험 추적/버저닝, 분산 학습, AutoML)

또한 이 장은 알고리즘 자체(로지스틱 회귀, 트리, KNN, 신경망 등)의 동작 원리를 깊게 설명하기보다는, 알고리즘을 둘러싼 개발/선정/평가 기법을 다룹니다.

## Evaluating ML Models: 어떤 알고리즘을 선택할 것인가
실제 문제에는 가능한 해법이 많습니다. 같은 문제에서도
- “익숙한 로지스틱 회귀로 시작할지”
- “최신 SOTA 모델을 바로 적용할지”
- “동료가 추천한 트리 기반 모델을 따를지”
  같은 선택이 발생합니다.

이론적으로는 모든 알고리즘을 다 실험해 보면 되지만, 현실에서는 시간과 컴퓨팅 자원이 제한되어 있으므로 전략적으로 모델 후보를 좁혀야 합니다.

### 클래식 ML vs 딥러닝: “대체”가 아니라 “공존”
딥러닝이 지난 10년간 큰 진전을 이끌었고 주목도도 높지만, 그렇다고 클래식 ML이 사라지는 것은 아닙니다.

- 추천 시스템은 여전히 협업 필터링/행렬 분해 같은 기법을 많이 사용합니다.
- 지연(latency)이 엄격한 분류 업무에서는 트리 기반(특히 GBDT)이 여전히 강력한 선택지입니다.
- 실제 프로덕션에서는 신경망과 클래식 ML을 함께 쓰는 조합도 흔합니다.
  - 예: 신경망 + 결정트리를 앙상블로 결합
  - 예: k-means로 특징을 추출해 신경망 입력으로 사용
  - 예: BERT/GPT 계열로 임베딩을 만든 뒤 로지스틱 회귀로 분류

모델 선택은 “신경망 vs 클래식”의 단순 이분법이 아니라, 문제/제약/운영환경에 맞는 구성으로 접근해야 합니다.

### 문제 유형에 따라 후보 모델군이 달라진다
현업에서는 “세상의 모든 모델 중 하나”를 고르기보다, 문제 유형에 맞는 적절한 후보군을 먼저 설정합니다.

- 유해 트윗 탐지 → 텍스트 분류  
  후보: Naive Bayes, Logistic Regression, RNN, Transformer(BERT/GPT 계열)
- 이상 거래 탐지 → 이상치(Abnormality) 탐지  
  후보: KNN, Isolation Forest, Clustering, Neural Networks

이 과정에서 일반적인 ML 태스크와 전형적 접근법에 대한 지식이 중요합니다.

### 성능만 보지 말고 시스템 속성까지 함께 본다
모델 선택 시 단순히 accuracy/F1/log loss 같은 오프라인 지표뿐 아니라 다음 속성을 함께 고려해야 합니다.

- 라벨(정답) 필요량: 어떤 모델은 많은 라벨을 요구하고, 어떤 모델은 적은 라벨로도 시작이 가능합니다.
- 학습 비용: 학습 시간/컴퓨팅 자원/개발 리소스
- 추론 비용: inference latency, CPU/GPU 요구
- 해석가능성: 왜 그렇게 예측했는지 설명 가능한가

예를 들어 로지스틱 회귀는 복잡한 신경망보다 정확도가 낮을 수 있어도,
- 적은 라벨로 시작 가능
- 학습이 빠르고 배포가 쉬움
- 설명이 용이함
  같은 장점으로 인해 프로덕션에서 매우 현실적인 선택이 될 수 있습니다.

### “알고리즘 비교”는 금방 낡는다
알고리즘의 유행과 주력은 빠르게 바뀝니다. 2016년에는 LSTM 기반 RNN이 NLP에서 주류였지만, 불과 2년 뒤 Transformer가 큰 폭으로 대체했습니다.  
책에서는 “최고의 비교표”를 제공하기보다는, 기본기를 갖추고 직접 실험을 통해 이해하는 것을 권합니다. 최신 흐름을 따라가기 위해 주요 학회(NeurIPS/ICLR/ICML) 트렌드 모니터링도 도움이 됩니다.

## Six tips for model selection: 모델 선택을 돕는 6가지 팁
### Tip 1) Avoid the state-of-the-art trap: SOTA 함정 피하기
SOTA 모델은 보통 정적 데이터셋에서의 성능으로 “최신”이 된 경우가 많습니다. 하지만 그것이
- 내 데이터에서도 더 잘될지
- 충분히 빠르고/저렴하게 운영 가능한지
- 구현/배포 비용이 감당 가능한지
  를 보장하지 않습니다.

핵심은 “최신”이 아니라 문제를 실제로 해결하는 해법이며, 더 싸고 단순한 해결책이 문제를 충분히 해결한다면 그쪽을 택하는 것이 합리적입니다.

### Tip 2) Start with the simplest models: 가장 단순한 모델부터 시작하기
단순함은 세 가지 이점을 줍니다.
1) 배포를 빨리 해볼 수 있어, 학습 파이프라인과 예측 파이프라인의 일관성을 조기 검증할 수 있습니다.
2) 복잡도를 단계적으로 늘리면 디버깅과 원인 분석이 쉬워집니다.
3) 단순 모델이 베이스라인이 되어 이후 개선의 기준점이 됩니다.

다만 “단순 모델”이 꼭 “시작하기 쉬운 모델”과 동일하진 않습니다. 예를 들어 pretrained BERT는 구조적으로는 복잡하지만, Hugging Face 같은 생태계 덕분에 시작 난이도는 낮을 수 있습니다.  
그럼에도 pretrained BERT가 정말 더 나은지 확인하기 위해, 더 단순한 모델들도 비교 실험해 보는 것이 권장됩니다. 또한 pretrained 모델은 시작은 쉬워도 “그 이상으로 개선”하는 비용이 커질 수 있습니다.

### Tip 3) Avoid human biases: 사람의 편향을 제거하기
모델 비교는 실험 설계에 따라 결과가 크게 달라집니다. 어떤 아키텍처에 더 흥미가 있는 사람은 그 모델에 더 많은 시간(피처/하이퍼파라미터 탐색)을 투자해 더 좋은 결과를 얻을 가능성이 큽니다.

따라서 서로 다른 아키텍처를 비교할 때는 비교 가능한 수준의 노력과 예산으로 실험을 맞추는 것이 중요합니다.  
“한쪽은 100번 실험, 다른 쪽은 3번 실험”은 공정한 비교가 아닙니다.

### Tip 4) Evaluate good performance now vs later: 지금의 최선 vs 미래의 최선
지금 가장 좋은 모델이 2개월 뒤에도 가장 좋을 필요는 없습니다.  
예를 들어 지금은 데이터가 적어서 트리 모델이 더 나을 수 있지만, 데이터가 늘어나면 신경망이 더 좋아질 수도 있습니다.

이를 가늠하는 한 방법이 러닝 커브(learning curve)입니다.  
러닝 커브는 “학습 샘플 수”에 따른 “훈련/검증 성능 변화”를 보여주며, 데이터가 늘어날 때 성능이 더 개선될 여지가 있는지 감을 줍니다(정확한 개선폭 예측은 어렵더라도 방향성은 제공합니다).

책에서는 실제로
- 오프라인에서는 협업 필터링이 더 좋았지만
- 프로덕션에서 신경망을 지속 학습시켜
- 2주 뒤 신경망이 역전한 사례
  를 들어, “업데이트 용이성/개선 잠재력”도 모델 선택의 중요한 축임을 강조합니다.

### Tip 5) Evaluate trade-offs: 트레이드오프를 평가하기
모델 선택은 항상 절충입니다.
- FP vs FN: 업무 특성에 따라 무엇이 더 위험한지 다릅니다.
  - 지문 잠금 해제: FP(침입 허용)가 더 위험 → FP를 줄이는 방향 선호
  - COVID-19 스크리닝: FN(환자 놓침)이 더 위험 → FN을 줄이는 방향 선호
- 정확도 vs 계산비용/지연: 더 복잡한 모델은 더 정확할 수 있지만 GPU가 필요하거나 지연이 커질 수 있습니다.
- 성능 vs 해석가능성: 복잡한 모델은 강력하지만 설명이 어려울 수 있습니다.

즉, “정확도 최고”가 아니라 시스템 목표에 맞는 균형점을 찾아야 합니다.

### Tip 6) Understand your model’s assumptions: 모델 가정 이해하기
George Box의 말처럼 “모든 모델은 틀리지만, 어떤 모델은 유용합니다.” 모델은 세계를 가정으로 단순화합니다.  
각 모델의 가정을 이해하고, 내 데이터가 그 가정을 얼마나 만족하는지 보는 것이 모델 선택에 도움이 됩니다.

책에서 예시로 든 가정들은 다음과 같습니다.
- Prediction assumption: X로부터 Y를 예측할 수 있다고 가정
- IID: 샘플이 독립 동일 분포에서 나왔다고 가정
- Smoothness: 비슷한 입력은 비슷한 출력을 낸다고 가정
- Tractability: 생성 모델에서 $$P(Z|X)$$ 계산이 가능하다고 가정
- Boundaries: 선형 분류기는 결정 경계가 선형이라고 가정
- Conditional independence: Naive Bayes는 클래스 조건부로 특성들이 독립이라고 가정
- Normally distributed: 많은 통계 방법이 정규성을 가정


## Ensembles: 성능을 올리는 고전적이면서 강력한 방법
단일 모델을 만든 뒤 성능을 더 올리고 싶다면, 꾸준히 성능 향상을 주는 대표 기법이 앙상블(ensemble)입니다. 앙상블은 여러 모델(=base learner)의 출력을 결합해 최종 예측을 만듭니다.

- 분류: 다수결(majority vote)
- 회귀: 평균(average)

Kaggle, SQuAD 등의 사례에서 상위 해법 상당수가 앙상블을 사용했음을 언급하며, “조금의 성능 향상이 큰 금전적 이득으로 연결되는 영역(예: CTR 예측)”에서는 프로덕션에서도 앙상블이 여전히 쓰인다고 설명합니다.  
다만 앙상블은 배포/유지보수 복잡도가 증가하기 때문에 모든 상황에서 선호되지는 않습니다.

### 왜 앙상블이 통하는가: “비상관성”이 핵심
책은 직관을 위해 간단한 예시를 듭니다.

- 정확도 70%인 스팸 분류기 3개가 있고,
- 각 분류기의 예측이 서로 독립(비상관)이라고 가정하면,
- 다수결 앙상블의 정확도는 78.4%가 됩니다.

핵심은 베이스 러너 간 상관(correlation)이 낮을수록 앙상블 효과가 커진다는 점입니다.  
모델들이 완전히 동일하게 틀리는(완전 상관) 경우, 앙상블은 이득이 거의 없습니다.

그래서 실무에서는 서로 다른 유형의 모델을 섞어 상관을 낮추는 전략이 흔합니다.
- 예: Transformer + RNN + GBDT 조합

또한 앙상블(특히 bagging/boosting)과 리샘플링은 불균형 데이터에서도 도움이 될 수 있다는 조사 연구들을 함께 언급합니다.

## 앙상블을 만드는 3가지 방법: Bagging / Boosting / Stacking
### Bagging (Bootstrap Aggregating)
Bagging은 학습 안정성과 정확도를 높이기 위해, 원 데이터에서 복원추출(sampling with replacement)로 여러 부트스트랩 데이터셋을 만들고 각 데이터셋에 모델을 따로 학습시킨 뒤 결합합니다.

- 분류: 다수결
- 회귀: 평균

Bagging은 분산(variance)을 줄이고 과적합을 완화하는 방향으로 동작하며, 특히
- 신경망
- 결정트리(분류/회귀 트리)
  처럼 “불안정한(variance가 큰)” 방법에서 효과가 큽니다.  
  반면 KNN처럼 비교적 안정적인 방법에서는 성능이 약간 저하될 수도 있다고 설명합니다.

또한 랜덤 포레스트는 bagging + 피처 랜덤성을 결합한 대표 예시입니다(각 트리가 임의 부분 피처만 사용).

### Boosting
Boosting은 약한 학습기(weak learner)를 단계적으로 결합해 강한 학습기로 만드는 반복적(Iterative) 앙상블입니다.

핵심 아이디어는 다음과 같습니다.
1) 첫 번째 약한 모델 학습
2) 오분류된 샘플에 더 큰 가중치를 부여(재가중)
3) 재가중 데이터로 다음 모델 학습
4) 앙상블 전체의 오분류 샘플에 더 집중하도록 반복
5) 최종적으로 각 모델을 가중 결합(오차가 작은 모델에 더 큰 가중치)

대표 예시로 GBM(Gradient Boosting Machine), 그 변형으로 XGBoost가 언급됩니다. 또한 LightGBM은 분산 학습과 병렬화를 통해 대규모 데이터에서 더 빠른 학습을 가능하게 해, 최근에는 LightGBM을 택하는 경우도 많다고 연결합니다.

### Stacking
Stacking은
1) 훈련 데이터로 여러 base learner를 학습한 뒤
2) base learner들의 출력(예측값)을 입력으로 받아
3) 최종 예측을 만드는 meta-learner를 학습하는 방식입니다.

meta-learner는 단순히
- 분류: 다수결
- 회귀: 평균
  일 수도 있고, 로지스틱 회귀/선형 회귀 같은 별도 모델이 될 수도 있습니다.


## 맺음말
> 피처 설계 이후의 단계인 모델 개발과 선택을 “알고리즘 자체”가 아니라 시스템 관점의 판단 기준으로 정리합니다. 
> 모델 선택에서는 SOTA에 대한 환상을 경계하고, 단순한 베이스라인에서 출발해 공정한 실험으로 비교하며, 미래의 데이터 증가와 개선 잠재력까지 고려해야 합니다. 
> 정확도만이 아니라 지연, 비용, 해석가능성, FP/FN 위험도 같은 트레이드오프가 곧 시스템 품질을 좌우합니다. 마지막으로 앙상블은 여전히 강력한 성능 개선 수단이며, 특히 베이스 러너 간 상관을 낮추는 설계가 효과의 핵심입니다.
> 좋은 모델 개발은 “좋은 알고리즘 찾기”가 아니라, 제한된 자원 속에서 지속적으로 비교·개선·검증 가능한 개발 루프를 설계하는 일에 가깝습니다.
