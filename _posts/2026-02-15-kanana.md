---
title: Kanana-Nano 2.1B Inference Benchmark on Korean Summarization
date: 2026-02-15 12:15:00 +09:00
categories: ['llm']
tags: ['llm', 'korean-llm', 'nlp', 'benchmark', 'gpu-inference']
---

본 글에서는 Kakao에서 공개한 Kanana-Nano 2.1B 모델을 실제 GPU 환경에서 구동하며, 자원 사용량과 한국어 요약 성능을 간단히 확인한 실험 기록을 정리합니다.  
목적은 모델 소개가 아니라, 경량 한국어 LLM이 연구 및 개발 환경에서 어느 정도 실용적인지 직접 확인하는 데 있습니다.

## 1. 실험 동기

최근 공개된 한국어 중심 LLM 중 Kanana 시리즈가 “경량 + 한국어 성능” 측면에서 언급되는 것을 보고, 실제로 로컬 GPU 환경에서 어느 정도 부담 없이 사용할 수 있는지 확인해보고자 했습니다.

특히 다음을 확인하는 데 관심이 있었습니다.

- 2.1B 모델의 실제 VRAM 요구량  
- 생성 길이에 따른 추론 시간 변화  
- KV cache 사용 여부의 영향  
- 논문 초록 요약에서의 기본 품질


## 2. 실험 환경

- Model: `kakaocorp/kanana-nano-2.1b-base`  
- GPU: NVIDIA RTX 3090 (24GB)  
- Precision: bfloat16  
- Framework: HuggingFace Transformers  
- Decoding: greedy (`do_sample=False`)  
- Batch size: 1  
- Task: 논문 초록 한국어 요약

모든 실험은 단일 GPU, 단일 요청 기준으로 수행했습니다.

## 3. 모델 로딩 시 메모리 사용량

모델 로딩 직후 CUDA 메모리 사용량은 다음과 같았습니다.

```

allocated     : 3.93 GB
reserved      : 4.06 GB
max_allocated : 3.93 GB

```

2.1B 파라미터 모델임을 고려하면 약 4GB 내외의 VRAM으로 로딩 가능하다는 점은 인상적이었습니다.  
개인 GPU 환경에서도 실험 가능한 수준입니다.

## 4. 생성 속도 및 메모리 벤치마크

Transformer 논문 초록 일부를 입력으로 사용하여, 생성 토큰 수와 KV cache 사용 여부에 따른 변화를 측정했습니다.

| Setting | Tokens | KV Cache | Time (ms) | Peak VRAM |
|--------|--------|----------|-----------|-----------|
| short | 32 | ON | 1260 | 3.96 GB |
| short | 80 | ON | 2065 | 3.96 GB |
| short | 160 | ON | 3183 | 3.97 GB |
| short | 80 | OFF | 4220 | 3.95 GB |
| long input | 80 | ON | 2126 | 4.02 GB |

### 관찰

#### KV cache 영향
KV cache를 비활성화하면 추론 시간이 거의 2배 가까이 증가했습니다.  
반복 추론이 필요한 환경에서는 cache 사용이 사실상 필수적이라고 볼 수 있습니다.

#### 입력 길이 영향
입력 길이를 크게 늘려도 VRAM 증가는 제한적이었습니다.  
단일 요청 기반 추론에서는 입력 길이가 주요 병목이 되지 않는 것으로 보입니다.

#### 생성 길이와 시간
생성 토큰 수에 비례하여 추론 시간이 비교적 선형적으로 증가했습니다.  
이는 예상 가능한 동작이었습니다.

---

## 5. 실험 중 겪은 시행착오

초기 설정에서는 요약이 중간에서 자주 끊기는 문제가 있었습니다.

`max_new_tokens=80` 설정에서는 모델이 프롬프트 일부를 재출력하며 토큰을 소모했고, 그 결과 요약이 완결되기 전에 종료되는 경우가 발생했습니다.  
이후 160~256 토큰으로 늘리면서 대부분 안정적으로 마무리되는 출력을 얻을 수 있었습니다.

또한 base 모델 특성상 지시문을 그대로 따라쓰는 경향이 있어,  
“요약만 출력하라”는 제약을 프롬프트에 명시했을 때 출력 품질이 더 안정적이었습니다.

---

## 6. 요약 결과 예시

입력은 “Attention Is All You Need” 논문 초록 일부입니다.

출력 요약:

> 기존의 시퀀스 전달 모델은 순환 또는 컨볼루션 신경망 기반이었으며 인코더-디코더 구조를 사용했습니다.  
> 고성능 모델은 attention으로 두 구조를 연결했습니다.  
> 본 연구는 순환과 컨볼루션을 제거하고 attention만 사용하는 Transformer를 제안했습니다.

### 간단한 품질 평가

- 핵심 내용 보존: 양호  
- 한국어 자연스러움: 무난한 수준  
- 명백한 환각: 관찰되지 않음  

대형 상용 모델 수준의 정교함은 아니지만, 경량 모델 기준으로는 안정적인 요약을 생성했습니다.

---

## 7. 활용 가능성에 대한 생각

Kanana-Nano 2.1B는 다음과 같은 용도에서 실용적일 수 있다고 판단됩니다.

- 한국어 RAG 시스템의 생성 모델  
- 연구용 베이스라인 모델  
- 자원 제약 환경에서의 프로토타이핑  
- 교육 및 실험 목적의 로컬 LLM

초대형 모델을 대체할 수준은 아니지만,  
“가벼운 한국어 LLM”이라는 목적에는 충분히 부합합니다.

## 맺음말

Kanana-Nano 2.1B는 다음과 같은 특징을 보였습니다.

- 약 4GB VRAM으로 구동 가능한 경량 모델  
- 한국어 요약에서 안정적인 기본 성능  
- KV cache 사용 시 실용적인 추론 속도  

> 한국어 중심 작업에서 경량 LLM을 찾는 경우, 한 번쯤 실험해볼 만한 모델이라고 생각됩니다.

> 추후에는 RAG 환경이나 도메인 특화 데이터에서의 동작도 확인해볼 계획입니다.
